0127and0128

+ 昨天学习了一天，今天就下午学习了，别这么快就怠惰吧。。。回来的家里人多了，不能呆奶奶家了，有点闹，加上剪了这个头发，看一次镜子，就想一次学习。。。真是的，不剪头发是流氓，剪了头发说我像劳改犯，，，
+ 上午睡到8点10分，还想睡，但还是起来了
+ 刚坐下，那边打电话又要去提东西了，双胞胎三姑四姑到门了，果然，第一句话，好瘦啊，还是那个样子，大学里面没吃吗，en......还行吧，这种对话从小学就开始了，就是地点换了一下，其他雷同，内心毫无波动个，甚至想学个习
+ 20个单词，强行刷了一套6级，阅读错一半？？？？？？？？？？我英语这么垃圾吗？？？？？
+ 下午看了一节机器学习，没啥新东西，但是感觉我在和英语打架。。。
+ 晚上没去奶奶家吃饭，中午吃太多了
+ 支付宝五福互相送，战报：一次沾了南方的敬业福，俊华没沾到我的，瓜浩第一法敬业福，第二发花花卡，，，
+ 11点20了，好饿，突然想下楼吃个炒粉，，，





### 笔记

+ **模型表示**：

描述稍微正式的监督学习问题,我们的目标是,给定一个训练集,学习函数h:X→Y这h(X)是一个“好”的对应值预测Y。

![mark](http://media.sumblog.cn/blog/20190128/R7kEOAwn4sxC.PNG) 

和之前讲的一样，当我们试图预测的目标变量是连续的，比如在我们的住房例子中，我们把学习问题称为回归问题。当y只能取少量离散值时(例如，给定居住面积，我们想要预测住宅是房子还是公寓，我们称之为分类问题

+ **代价函数（成本函数）**

我们可以用代价函数来衡量假设函数的准确性。它取所有假设（ hypothesis）结果的平均值加上x的输入和y的实际输出。

![mark](http://media.sumblog.cn/blog/20190128/U4axSVe9Qz5M.PNG)

其实就是我们学过的线性规划的知识，求出最吻合的θ1，θ0，得到尽可能得到最好的直线，即散点到直线的平均垂直距离的平方是最小的。



+ **梯度下降算法**



###### #我们有必要先来了解下等高线图





**等高线图**是包含许多等高线的图形。两个变量函数的等高线在同一直线的所有点上都有一个常数值。



先直接上个图







![mark](http://media.sumblog.cn/blog/20190128/69LYOR3chDFn.PNG)

 取任意颜色并沿着“圆”移动，我们期望得到相同的代价函数值。例如,三个绿色点发现上面的绿线具有相同的价值J(θ0,θ1),因此,他们沿着同一条直线被发现。绕x显示成本函数的值时,左边的图θ0 = 800和θ1 = -0.15。取另一个h(x)画出等高线，得到如下图

![mark](http://media.sumblog.cn/blog/20190128/f8gV31WA3YqM.PNG)

当θ0 = 360和θ1 = 0,价值的J(θ0,θ1)等高线图接近中心从而减少成本函数的错误。假设函数斜率为正会得到更好的拟合结果假设函数斜率为正会得到更好的拟合结果。



![mark](http://media.sumblog.cn/blog/20190128/G79iNL8pbPVV.PNG)



上图尽可能最小化代价函数,因此θ1，θ0的结果往往是分别约为0.12和250年的。把这些值画在我们的图的右边几乎把我们的点放在最里面的“圆”的中心。







OK接下来，我们把θ0当x轴和θ1当y轴,垂直z轴上是成本函数。图上的这些点将是代价函数的结果使用这些特定参数的假设



![mark](http://media.sumblog.cn/blog/20190128/B47kVu1iYzUn.PNG)



例如,每一个点之间的距离在上面的图中代表一个步骤取决于我们参数α。一个较小的α将导致一个更小的移动,更大的α导致一个更大的一步。方向采取的步骤是确定的偏导数J (θ0,θ1)。根据图上的起始点，可以得到不同的点。



![mark](http://media.sumblog.cn/blog/20190128/h2j6TCpwDz9E.PNG)

我们要做的及时重复这个步骤直到收敛 （到达最低点）



需要注意一下这个参数α，不合适的α可能会造成没有办法收敛或者出问题（下图以某个方向的偏导为例）



![mark](http://media.sumblog.cn/blog/20190128/dIewuDmFjSgc.PNG)









当具体应用于线性回归的情况下，可以推导出一种新的形式的梯度下降方程。我们可以把实际成本函数和假设函数代入方程

![mark](http://media.sumblog.cn/blog/20190128/kYepnE3rH3tM.PNG)



![mark](http://media.sumblog.cn/blog/20190128/mrirJhFWGdcW.PNG)

这就是原始代价函数j上的梯度下降法。这种方法每一步都要看整个训练集中的每一个例子，称为批量梯度下降法。注意，虽然梯度下降在一般情况下容易受到局部极小值的影响，但我们在这里提出的线性回归优化问题只有一个全局最优解，没有其他局部最优解。